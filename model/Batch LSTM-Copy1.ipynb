{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuloucn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#from model_embeddings import ModelEmbeddings\n",
    "from evaluator import Evaluator\n",
    "from vocab import Vocab, VocabEntry\n",
    "from utils import read_corpus, pad_sents, batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = []\n",
    "unparsed_definition = []\n",
    "words = []\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_train_words.txt') as f:\n",
    "    words += f.read().splitlines()\n",
    "    \n",
    "with open('../data/data_train_definitions.txt') as f:\n",
    "    unparsed_definition += f.read().splitlines()\n",
    "    definitions += [word_tokenize(a) for a in unparsed_definition]\n",
    "    \n",
    "training_data = [(definitions[i], words[i]) for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(words) == len(definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word types: 23452, number of word types w/ frequency >= 0: 23452\n",
      "28572\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluator()\n",
    "glove_dict = eval.load_glove_embeddings(max_line = 100000)\n",
    "\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')\n",
    "vocab = VocabEntry.from_corpus(src_sents, 1000000, 0)\n",
    "\n",
    "for word in words:\n",
    "    vocab.add(word)\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, src_pad_token_idx, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, src_pad_token_idx)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix)) #figure out what is here\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab, glove_dict):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (VocabEntry)\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        matrix_len = len(vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, self.embed_size))\n",
    "        words_found = 0\n",
    "        #print(len(vocab), weights_matrix.shape)\n",
    "        for word, index in vocab.word2id.items():\n",
    "            try:\n",
    "                weights_matrix[index] = np.array(glove_dict[word])\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[index] = np.random.normal(scale=0.6, size=(self.embed_size,))\n",
    "\n",
    "        # default values\n",
    "        src_pad_token_idx = vocab['<pad>']\n",
    "        self.source = create_emb_layer(weights_matrix, src_pad_token_idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab, glove_dict):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = ModelEmbeddings(input_size, vocab, glove_dict)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        \n",
    "        self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias = True)\n",
    "\n",
    "    def forward(self, input_, hidden, lengths , dropout_rate = 0.3):\n",
    "        embedded = self.embedding.source[0](input_)\n",
    "        embedded = pack_padded_sequence(embedded, lengths)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        dropout = nn.Dropout(dropout_rate)\n",
    "        hidden_dropped = dropout(hidden.permute(1,0,2)) # you dont need dropout in validation\n",
    "        projected = self.linear(hidden_dropped)\n",
    "        return projected, hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device = None):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderRNN(50, 50, vocab, glove_dict)\n",
    "loss_function = nn.SmoothL1Loss(reduction = \"sum\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check overfit\n",
    "\n",
    "# definition_indices = vocab.words2indices(definitions)\n",
    "# words_in = 0\n",
    "# words_out = 0\n",
    "\n",
    "# import timeit\n",
    "# start = timeit.default_timer()\n",
    "# losses = []\n",
    "\n",
    "# batch_size = 256\n",
    "\n",
    "# for src_sents, tgt_word in batch_iter(training_data, batch_size, True):\n",
    "#     for i in range(5000):\n",
    "#         model.zero_grad()\n",
    "#         x_lengths = [len(sent) for sent in src_sents]\n",
    "#         x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "#         init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "#         tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "#         y_array = model.embedding.source[0](torch.tensor(vocab.words2indices(tgt_word))).double()\n",
    "#         y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "#         loss = loss_function(y_pred, y_array)\n",
    "#         loss.backward()\n",
    "#         optimizer.step() \n",
    "#         losses.append(loss)\n",
    "#         if i % 100 == 0:\n",
    "#             print(epoch, loss)\n",
    "#     break\n",
    "    \n",
    "# stop = timeit.default_timer()\n",
    "\n",
    "# print('Time: ', stop - start)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(plt.plot([l.double() for l in losses][:1200]))\n",
    "\n",
    "# model.zero_grad()\n",
    "# x_lengths = [len(sent) for sent in src_sents]\n",
    "# x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "# init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "# tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "# y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "\n",
    "# print(y_pred.shape)\n",
    "# for i in range(len(y_pred)):\n",
    "#     eval.top_ten_hundred(validate_dict, tgt_word[i], y_pred[i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(9929.5731, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "1 tensor(9722.5172, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "2 tensor(9432.3986, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "3 tensor(9672.4610, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "4 tensor(9355.1484, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "5 tensor(9280.2919, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "6 tensor(9405.4431, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "7 tensor(9203.3021, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "8 tensor(9418.9172, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "9 tensor(9447.1026, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "10 tensor(9238.8751, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "11 tensor(9284.6304, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "12 tensor(9316.2534, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "13 tensor(9300.5863, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "14 tensor(9342.8948, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "15 tensor(9374.8257, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "16 tensor(9464.5986, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "17 tensor(9354.3166, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "18 tensor(9365.4685, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "19 tensor(9237.5725, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "20 tensor(9315.6670, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "21 tensor(9301.8682, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "22 tensor(9293.7489, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "23 tensor(9360.0814, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "24 tensor(9311.1286, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "25 tensor(9358.4165, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "26 tensor(9330.4409, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "27 tensor(9492.2582, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "28 tensor(9324.3237, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "29 tensor(9396.0030, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "30 tensor(9346.5121, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "31 tensor(9282.7293, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "32 tensor(9462.5550, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "33 tensor(9352.3550, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "34 tensor(9416.6475, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "35 tensor(9233.2680, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "36 tensor(9451.3529, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "37 tensor(9292.5369, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "38 tensor(9295.8359, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "39 tensor(9184.2943, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "40 tensor(9466.9374, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "41 tensor(9308.6043, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "42 tensor(9301.7253, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "43 tensor(9112.3105, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "44 tensor(9313.3812, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "45 tensor(9376.0370, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "46 tensor(9318.5996, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "47 tensor(9593.4705, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "48 tensor(9268.6864, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "49 tensor(9206.1737, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "50 tensor(9387.9976, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "51 tensor(9263.1434, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "52 tensor(9273.9087, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "53 tensor(9241.6268, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "54 tensor(9434.7689, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "55 tensor(9225.2324, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "56 tensor(9166.3408, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "57 tensor(9077.6221, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "58 tensor(9315.2509, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "59 tensor(9204.4235, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "60 tensor(9111.9716, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "61 tensor(9194.0703, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "62 tensor(9282.4660, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "63 tensor(9239.0763, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "64 tensor(9442.9902, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "definition_indices = vocab.words2indices(definitions)\n",
    "words_in = 0\n",
    "words_out = 0\n",
    "\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "losses = []\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "for epoch in range(50000):\n",
    "    for src_sents, tgt_word in batch_iter(training_data, batch_size, True):\n",
    "        model.zero_grad()\n",
    "        x_lengths = [len(sent) for sent in src_sents]\n",
    "        x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "        init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "        tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "        y_array = model.embedding.source[0](torch.tensor(vocab.words2indices(tgt_word))).double()\n",
    "        y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "        loss = loss_function(y_pred, y_array)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    losses.append(loss)\n",
    "    print(epoch, loss)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(plt.plot([l.double() for l in losses][:1200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
