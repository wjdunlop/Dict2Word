{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#from model_embeddings import ModelEmbeddings\n",
    "from evaluator import Evaluator\n",
    "from vocab import Vocab, VocabEntry\n",
    "from utils import read_corpus, pad_sents\n",
    "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = []\n",
    "unparsed_definition = []\n",
    "words = []\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_train_words.txt') as f:\n",
    "    words += f.read().splitlines()\n",
    "    \n",
    "with open('../data/data_train_definitions.txt') as f:\n",
    "    unparsed_definition += f.read().splitlines()\n",
    "    definitions += [word_tokenize(a) for a in unparsed_definition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(words) == len(definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word types: 23452, number of word types w/ frequency >= 0: 23452\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluator()\n",
    "glove_dict = eval.load_glove_embeddings(max_line = 50000)\n",
    "\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')\n",
    "vocab = VocabEntry.from_corpus(src_sents, 30000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, src_pad_token_idx, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, src_pad_token_idx)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix)) #figure out what is here\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab, glove_dict):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (VocabEntry)\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        matrix_len = len(vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, self.embed_size))\n",
    "        words_found = 0\n",
    "        #print(len(vocab), weights_matrix.shape)\n",
    "        for word, index in vocab.word2id.items():\n",
    "            try:\n",
    "                weights_matrix[index] = np.array(glove_dict[word])\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[index] = np.random.normal(scale=0.6, size=(self.embed_size,))\n",
    "\n",
    "        # default values\n",
    "        src_pad_token_idx = vocab['<pad>']\n",
    "        self.source = create_emb_layer(weights_matrix, src_pad_token_idx, True)\n",
    "        ### END YOUR CODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab, glove_dict):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = ModelEmbeddings(input_size, vocab, glove_dict)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias = True)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        embedded = self.embedding.source[0](input_)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        projected = self.linear(hidden.permute(1,0,2))\n",
    "        return projected, hidden\n",
    "\n",
    "    def initHidden(self, device = None):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderRNN(50, 50, vocab, glove_dict)\n",
    "loss_function = nn.SmoothL1Loss(reduction = \"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.0055, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "1 tensor(0.0033, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "2 tensor(0.0079, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "3 tensor(0.0068, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n",
      "4 tensor(0.0040, dtype=torch.float64, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "definition_indices = vocab.words2indices(definitions)\n",
    "words_in = 0\n",
    "words_out = 0\n",
    "for epoch in range(500000):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for i in range(len(definition_indices)): #should be range(len(definition_indices))\n",
    "        if words[i] not in glove_dict: \n",
    "            continue\n",
    "        model.zero_grad()\n",
    "        x = torch.tensor(definition_indices[i])\n",
    "\n",
    "        init_hidden = model.initHidden()\n",
    "        tag_scores = model.forward(x.view(x.shape[0], 1), init_hidden)\n",
    "        y_array = np.array(glove_dict[words[i]]) if words[i] in glove_dict else np.random.normal(scale=0.6, size=(50,))\n",
    "        y = torch.tensor(y_array).double()\n",
    "        y_pred = tag_scores[0].view((tag_scores[0].shape[2])).double()\n",
    "\n",
    "        loss = loss_function(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(epoch, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_dict = {}\n",
    "with open('../data/data_train_words.txt') as f:\n",
    "    lines = f.readlines()\n",
    "lines = [line[:-1] for line in lines]\n",
    "validate_words = sorted(list(set(lines)))\n",
    "for word in validate_words:\n",
    "    if word in glove_dict:\n",
    "        validate_dict[word] = glove_dict[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(200, 250): #len(definition_indices)\n",
    "    if count >= 100:\n",
    "        break\n",
    "    x = torch.tensor(definition_indices[i])\n",
    "    #print(x.shape)\n",
    "    init_hidden = model.initHidden()\n",
    "    tag_scores = model.forward(x.view(x.shape[0], 1), init_hidden)\n",
    "    #print(tag_scores[0].shape, y.shape)\n",
    "    if words[i] in validate_dict: \n",
    "        count += 1\n",
    "        #print(words[i])\n",
    "        y_array = np.array(glove_dict[words[i]]) if words[i] in glove_dict else np.random.normal(scale=0.6, size=(50,))\n",
    "        y = torch.tensor(y).double()\n",
    "        y_pred = tag_scores[0].view((tag_scores[0].shape[2])).double()\n",
    "        eval.top_ten_hundred(validate_dict, words[i], y_pred.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## print(eval.compute_th_accuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
