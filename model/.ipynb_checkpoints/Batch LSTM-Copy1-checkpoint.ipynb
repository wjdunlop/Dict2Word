{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuloucn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#from model_embeddings import ModelEmbeddings\n",
    "from evaluator import Evaluator\n",
    "from vocab import Vocab, VocabEntry\n",
    "from utils import read_corpus, pad_sents, batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = []\n",
    "unparsed_definition = []\n",
    "words = []\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_train_words.txt') as f:\n",
    "    words += f.read().splitlines()\n",
    "    \n",
    "with open('../data/data_train_definitions.txt') as f:\n",
    "    unparsed_definition += f.read().splitlines()\n",
    "    definitions += [word_tokenize(a) for a in unparsed_definition]\n",
    "    \n",
    "training_data = [(definitions[i], words[i]) for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(words) == len(definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word types: 23452, number of word types w/ frequency >= 0: 23452\n",
      "28572\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluator()\n",
    "glove_dict = eval.load_glove_embeddings(max_line = 100000)\n",
    "\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')\n",
    "vocab = VocabEntry.from_corpus(src_sents, 1000000, 0)\n",
    "\n",
    "for word in words:\n",
    "    vocab.add(word)\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, src_pad_token_idx, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, src_pad_token_idx)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix)) #figure out what is here\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab, glove_dict):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (VocabEntry)\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        matrix_len = len(vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, self.embed_size))\n",
    "        words_found = 0\n",
    "        #print(len(vocab), weights_matrix.shape)\n",
    "        for word, index in vocab.word2id.items():\n",
    "            try:\n",
    "                weights_matrix[index] = np.array(glove_dict[word])\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[index] = np.random.normal(scale=0.6, size=(self.embed_size,))\n",
    "\n",
    "        # default values\n",
    "        src_pad_token_idx = vocab['<pad>']\n",
    "        self.source = create_emb_layer(weights_matrix, src_pad_token_idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab, glove_dict):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = ModelEmbeddings(input_size, vocab, glove_dict)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias = False)\n",
    "\n",
    "    def forward(self, input_, hidden, lengths):\n",
    "        embedded = self.embedding.source[0](input_)\n",
    "        embedded = pack_padded_sequence(embedded, lengths)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        print(\"before\", output[0].shape)\n",
    "        output, _ = pad_packed_sequence(output)\n",
    "        print(\"after\", output[0].shape)\n",
    "        projected = self.linear(hidden.permute(1,0,2))\n",
    "        return projected, hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device = None):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderRNN(50, 50, vocab, glove_dict)\n",
    "loss_function = nn.SmoothL1Loss(reduction = \"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before tensor([[-0.1972,  0.0740,  0.0598,  ..., -0.1167, -0.3241, -0.2236],\n",
      "        [-0.2561, -0.0915,  0.0117,  ..., -0.1869, -0.1928, -0.1908],\n",
      "        [-0.1542, -0.0777, -0.1393,  ..., -0.2832, -0.1275, -0.2211],\n",
      "        ...,\n",
      "        [-0.1767,  0.2588, -0.3904,  ..., -0.0922, -0.2805, -0.2836],\n",
      "        [-0.4672,  0.0194, -0.1071,  ..., -0.1755, -0.3842, -0.3940],\n",
      "        [-0.3899, -0.1375, -0.1852,  ..., -0.2971, -0.2513, -0.1511]],\n",
      "       grad_fn=<CatBackward>)\n",
      "after tensor([[-0.1972,  0.0740,  0.0598,  ..., -0.1167, -0.3241, -0.2236],\n",
      "        [-0.2561, -0.0915,  0.0117,  ..., -0.1869, -0.1928, -0.1908],\n",
      "        [-0.1542, -0.0777, -0.1393,  ..., -0.2832, -0.1275, -0.2211],\n",
      "        ...,\n",
      "        [ 0.2282, -0.1450,  0.1607,  ..., -0.3200,  0.1188,  0.0300],\n",
      "        [ 0.2394, -0.0607,  0.2097,  ..., -0.1027,  0.0502,  0.1278],\n",
      "        [ 0.2266,  0.2485,  0.0360,  ..., -0.2232, -0.3111, -0.3713]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "before tensor([[ 0.0703, -0.2964,  0.1920,  ..., -0.4197,  0.5294,  0.4298],\n",
      "        [ 0.0599, -0.2319,  0.2173,  ..., -0.3713,  0.5366,  0.4706],\n",
      "        [ 0.0652, -0.1600,  0.1883,  ..., -0.4002,  0.4553,  0.4349],\n",
      "        ...,\n",
      "        [ 0.9595, -0.9999,  0.9994,  ..., -0.7905,  0.9940,  0.2964],\n",
      "        [ 0.9620, -0.9999,  0.9995,  ..., -0.8673,  0.9972,  0.6674],\n",
      "        [ 0.9669, -1.0000,  0.9996,  ..., -0.9210,  0.9990,  0.8096]],\n",
      "       grad_fn=<CatBackward>)\n",
      "after tensor([[ 0.0703, -0.2964,  0.1920,  ..., -0.4197,  0.5294,  0.4298],\n",
      "        [ 0.0599, -0.2319,  0.2173,  ..., -0.3713,  0.5366,  0.4706],\n",
      "        [ 0.0652, -0.1600,  0.1883,  ..., -0.4002,  0.4553,  0.4349],\n",
      "        ...,\n",
      "        [ 0.1970, -0.1228,  0.2249,  ..., -0.3965,  0.5891,  0.3881],\n",
      "        [ 0.3830, -0.2148,  0.4089,  ..., -0.3692,  0.5497,  0.3479],\n",
      "        [ 0.3849, -0.1824,  0.3187,  ..., -0.3870,  0.6591,  0.4012]],\n",
      "       grad_fn=<SelectBackward>)\n",
      "before tensor([[ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.]], grad_fn=<CatBackward>)\n",
      "after tensor([[ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        ...,\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.,  ..., -1.,  0.,  0.]], grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-80d61668680d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/local_nmt/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/local_nmt/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "definition_indices = vocab.words2indices(definitions)\n",
    "words_in = 0\n",
    "words_out = 0\n",
    "\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "losses = []\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "for epoch in range(50000):\n",
    "    for src_sents, tgt_word in batch_iter(training_data, batch_size, False):\n",
    "        model.zero_grad()\n",
    "        x_lengths = [len(sent) for sent in src_sents]\n",
    "        x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "        init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "        tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "        y_array = model.embedding.source[0](torch.tensor(vocab.words2indices(tgt_word))).double()\n",
    "        y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "        loss = loss_function(y_pred, y_array)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    losses.append(loss)\n",
    "    print(epoch, loss)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab, glove_dict):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = ModelEmbeddings(input_size, vocab, glove_dict)\n",
    "        self.gru = nn.GRU(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias = False)\n",
    "\n",
    "    def forward(self, input_, hidden):\n",
    "        embedded = self.embedding.source[0](input_)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        projected = self.linear(hidden.permute(1,0,2))\n",
    "        return projected, hidden\n",
    "\n",
    "    def initHidden(self, device = None):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderRNN(50, 50, vocab, glove_dict)\n",
    "loss_function = nn.SmoothL1Loss(reduction = \"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
