{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\willi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from model_embeddings import ModelEmbeddings\n",
    "from evaluator import Evaluator\n",
    "from vocab import Vocab, VocabEntry\n",
    "from utils import read_corpus, pad_sents, batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x9d in position 1799: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-97a8991d1290>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0munparsed_definition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msrc_sents\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/data_train_definitions.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\projects\\Dict2Word\\model\\utils.py\u001b[0m in \u001b[0;36mread_corpus\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m     52\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0msent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;31m# only append <s> and </s> to the target sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x9d in position 1799: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "definitions = []\n",
    "unparsed_definition = []\n",
    "words = []\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_train_words.txt') as f:\n",
    "    words += f.read().splitlines()\n",
    "    \n",
    "with open('../data/data_train_definitions.txt') as f:\n",
    "    unparsed_definition += f.read().splitlines()\n",
    "    definitions += [word_tokenize(a) for a in unparsed_definition]\n",
    "\n",
    "training_data = [(definitions[i], words[i]) for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999994it [01:06, 15032.04it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disheartenment\n",
      "disheartenment\n",
      "disheartenment\n",
      "disheartenment\n",
      "disheartenment\n",
      "siss\n",
      "siss\n",
      "siss\n",
      "siss\n",
      "siss\n",
      "unhorse\n",
      "unhorse\n",
      "unhorse\n",
      "unhorse\n",
      "splasher\n",
      "splasher\n",
      "splasher\n",
      "splasher\n",
      "brininess\n",
      "brininess\n",
      "despicableness\n",
      "despicableness\n",
      "despicableness\n",
      "despicableness\n",
      "entr'acte\n",
      "entr'acte\n",
      "entr'acte\n",
      "entr'acte\n",
      "fluidness\n",
      "fluidness\n",
      "fluidness\n",
      "fluidness\n",
      "perfecter\n",
      "perfecter\n",
      "perfecter\n",
      "perfecter\n",
      "perfecter\n",
      "perfecter\n",
      "butterfingered\n",
      "butterfingered\n",
      "butterfingered\n",
      "butterfingered\n",
      "butterfingered\n",
      "undersoil\n",
      "undersoil\n",
      "whitewood\n",
      "whitewood\n",
      "whitewood\n",
      "rosiness\n",
      "rosiness\n",
      "rosiness\n",
      "rosiness\n",
      "rosiness\n",
      "hoggish\n",
      "hoggish\n",
      "hoggish\n",
      "hoggish\n",
      "hoggish\n",
      "hoggish\n",
      "fleetly\n",
      "fleetly\n",
      "fleetly\n",
      "overshoe\n",
      "overshoe\n",
      "overshoe\n",
      "liquidambar\n",
      "liquidambar\n",
      "liquidambar\n",
      "plower\n",
      "plower\n",
      "plower\n",
      "plower\n",
      "pigfish\n",
      "pigfish\n",
      "pigfish\n",
      "pigfish\n",
      "eyehole\n",
      "eyehole\n",
      "eyehole\n",
      "bedevilment\n",
      "bedevilment\n",
      "bedevilment\n",
      "corrugate\n",
      "corrugate\n",
      "corrugate\n",
      "corrugate\n",
      "corrugate\n",
      "corrugate\n",
      "corrugate\n",
      "footmark\n",
      "footmark\n",
      "footmark\n",
      "egger\n",
      "egger\n",
      "egger\n",
      "egger\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "countercheck\n",
      "gunlock\n",
      "gunlock\n",
      "ament\n",
      "ament\n",
      "ament\n",
      "domiciliation\n",
      "domiciliation\n",
      "rutty\n",
      "rutty\n",
      "rutty\n",
      "rutty\n",
      "embossment\n",
      "embossment\n",
      "embossment\n",
      "embossment\n",
      "embossment\n",
      "embossment\n",
      "embossment\n",
      "embossment\n",
      "presbyopic\n",
      "presbyopic\n",
      "uncloak\n",
      "uncloak\n",
      "uncloak\n",
      "uncloak\n",
      "uncloak\n",
      "uncloak\n",
      "uncloak\n",
      "lopper\n",
      "lopper\n",
      "lopper\n",
      "tacker\n",
      "tacker\n",
      "defrauder\n",
      "defrauder\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "slue\n",
      "steaminess\n",
      "steaminess\n",
      "pinner\n",
      "pinner\n",
      "pinner\n",
      "nascency\n",
      "nascency\n",
      "nascency\n",
      "nascency\n",
      "nascency\n",
      "concuss\n",
      "concuss\n",
      "concuss\n",
      "concuss\n",
      "concuss\n",
      "concuss\n",
      "dawdler\n",
      "dawdler\n",
      "dawdler\n",
      "dawdler\n",
      "dawdler\n",
      "spellbind\n",
      "spellbind\n",
      "spellbind\n",
      "spellbind\n",
      "spellbind\n",
      "puniness\n",
      "puniness\n",
      "puniness\n",
      "puniness\n",
      "puniness\n",
      "puniness\n",
      "lithesome\n",
      "lithesome\n",
      "lithesome\n",
      "lithesome\n",
      "lithesome\n",
      "lithesome\n",
      "lithesome\n",
      "bosomed\n",
      "bosomed\n",
      "bosomed\n",
      "bosomed\n",
      "bosomed\n",
      "bosomed\n",
      "bosomed\n",
      "bosomed\n",
      "superannuate\n",
      "superannuate\n",
      "superannuate\n",
      "superannuate\n",
      "superannuate\n",
      "consolidative\n",
      "consolidative\n",
      "consolidative\n",
      "consolidative\n",
      "enamor\n",
      "enamor\n",
      "enamor\n",
      "enamor\n",
      "enamor\n",
      "closemouthed\n",
      "closemouthed\n",
      "closemouthed\n",
      "closemouthed\n",
      "closemouthed\n",
      "woefulness\n",
      "woefulness\n",
      "woefulness\n",
      "woefulness\n",
      "colligate\n",
      "colligate\n",
      "colligate\n",
      "colligate\n",
      "colligate\n",
      "tailboard\n",
      "tailboard\n",
      "tailboard\n",
      "hopple\n",
      "hopple\n",
      "hopple\n",
      "hopple\n",
      "hopple\n",
      "hopple\n",
      "hopple\n",
      "outride\n",
      "outride\n",
      "outride\n",
      "outride\n",
      "scrutinizer\n",
      "scrutinizer\n",
      "cragged\n",
      "cragged\n",
      "cragged\n",
      "cragged\n",
      "cragged\n",
      "voidance\n",
      "voidance\n",
      "voidance\n",
      "voidance\n",
      "voidance\n",
      "unlace\n",
      "unlace\n",
      "unlace\n",
      "unlace\n",
      "unlace\n",
      "unlace\n",
      "unlace\n",
      "tachymeter\n",
      "tachymeter\n",
      "tachymeter\n",
      "whitecap\n",
      "whitecap\n",
      "whitecap\n",
      "pricker\n",
      "pricker\n",
      "pricker\n",
      "pricker\n",
      "pricker\n",
      "pricker\n",
      "pricker\n",
      "pricker\n",
      "pricker\n",
      "solarize\n",
      "solarize\n",
      "solarize\n",
      "solarize\n",
      "befool\n",
      "befool\n",
      "befool\n",
      "befool\n",
      "befool\n",
      "befool\n",
      "befool\n",
      "stogy\n",
      "stogy\n",
      "stogy\n",
      "stogy\n",
      "stogy\n",
      "stogy\n",
      "excruciation\n",
      "excruciation\n",
      "excruciation\n",
      "haymow\n",
      "haymow\n",
      "haymow\n",
      "haymow\n",
      "globefish\n",
      "globefish\n",
      "towrope\n",
      "towrope\n",
      "amort\n",
      "amort\n",
      "amort\n",
      "amort\n",
      "amort\n",
      "amort\n",
      "gewgaw\n",
      "gewgaw\n",
      "gewgaw\n",
      "gewgaw\n",
      "gewgaw\n",
      "gewgaw\n",
      "gewgaw\n",
      "gewgaw\n",
      "packman\n",
      "packman\n",
      "packman\n",
      "madrona\n",
      "madrona\n",
      "madrona\n",
      "carbonize\n",
      "carbonize\n",
      "carbonize\n",
      "carbonize\n",
      "juiceless\n",
      "juiceless\n",
      "juiceless\n",
      "juiceless\n",
      "juiceless\n",
      "tastily\n",
      "tastily\n",
      "tastily\n",
      "tastily\n",
      "backbite\n",
      "backbite\n",
      "backbite\n",
      "backbite\n",
      "bejewel\n",
      "bejewel\n",
      "bejewel\n",
      "gnarl\n",
      "gnarl\n",
      "gnarl\n",
      "gnarl\n",
      "gnarl\n",
      "gnarl\n",
      "gnarl\n",
      "limper\n",
      "limper\n",
      "limper\n",
      "forehanded\n",
      "forehanded\n",
      "forehanded\n",
      "forehanded\n",
      "forehanded\n",
      "forehanded\n",
      "forehanded\n",
      "forehanded\n",
      "rathskeller\n",
      "rathskeller\n",
      "rathskeller\n",
      "dialyze\n",
      "dialyze\n",
      "dialyze\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n",
      "smatter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29064it [00:02, 14031.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word types: 23452, number of word types w/ frequency >= 0: 23452\n",
      "23456\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluator()\n",
    "fasttext_dict = eval.load_vectors(fname =\"../data/wiki-news-300d-1M-subword.vec\", max_line = 10000000)\n",
    "sub_fasttext_dict = {}\n",
    "#only train words in the dictionary\n",
    "for i in range(len(words)-1, -1, -1):\n",
    "    if words[i] not in fasttext_dict:\n",
    "        print(words[i])\n",
    "        words.pop(i)\n",
    "        definitions.pop(i)\n",
    "    else:\n",
    "        sub_fasttext_dict[words[i]] = fasttext_dict[words[i]]\n",
    "        \n",
    "high_freq_dict = eval.load_vectors(fname =\"../data/wiki-news-300d-1M-subword.vec\", max_line = 30000)\n",
    "sub_fasttext_dict.update()\n",
    "\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')\n",
    "vocab = VocabEntry.from_corpus(src_sents, 1000000, 0)\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/words_defs_dict.train\", \"wb\") as f:\n",
    "    pickle.dump((words, definitions, sub_fasttext_dict), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(words) == len(definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, src_pad_token_idx, non_trainable=False):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, src_pad_token_idx)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix)) #figure out what is here\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab, fasttext_dict):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (VocabEntry)\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        matrix_len = len(vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, self.embed_size))\n",
    "        words_found = 0\n",
    "        #print(len(vocab), weights_matrix.shape)\n",
    "        for word, index in vocab.word2id.items():\n",
    "            try:\n",
    "                weights_matrix[index] = np.array(fasttext_dict[word])\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[index] = np.random.normal(scale=0.6, size=(self.embed_size,))\n",
    "\n",
    "        # default values\n",
    "        src_pad_token_idx = vocab['<pad>']\n",
    "        self.source = create_emb_layer(weights_matrix, src_pad_token_idx, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRUModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, vocab, fasttext_dict):\n",
    "#         super(GRUModel, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embedding = ModelEmbeddings(input_size, vocab, fasttext_dict)\n",
    "#         self.gru = nn.GRU(input_size, hidden_size)\n",
    "        \n",
    "#         self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias = True)\n",
    "\n",
    "#     def forward(self, input_, hidden, lengths , dropout_rate = 0.3):\n",
    "#         embedded = self.embedding.source[0](input_)\n",
    "#         embedded = pack_padded_sequence(embedded, lengths)\n",
    "#         output, hidden = self.gru(embedded, hidden)\n",
    "#         dropout = nn.Dropout(dropout_rate)\n",
    "#         hidden_dropped = dropout(hidden.permute(1,0,2)) # you dont need dropout in validation\n",
    "#         projected = self.linear(hidden_dropped)\n",
    "#         return projected, hidden\n",
    "\n",
    "#     def initHidden(self, batch_size, device = None):\n",
    "#         return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab, fasttext_dict):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.vocab = vocab\n",
    "        self.embedding = ModelEmbeddings(input_size, vocab, fasttext_dict)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional = True)\n",
    "        self.linear = nn.Linear(self.hidden_size * 2, self.hidden_size, bias = True)\n",
    "\n",
    "    def forward(self, input_, hidden, lengths , dropout_rate = 0.3):\n",
    "        embedded = self.embedding.source[0](input_)\n",
    "        embedded = pack_padded_sequence(embedded, lengths)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        #print(h_n.shape, c_n.shape)\n",
    "        dropout = nn.Dropout(dropout_rate)\n",
    "        hidden_dropped = dropout(h_n.contiguous().view(1, -1, self.hidden_size * 2).permute(1,0,2)) # you dont need dropout in validation\n",
    "        projected = self.linear(hidden_dropped)\n",
    "        return projected, hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device = None):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bf05a19390a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfasttext_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mloss_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCosineEmbeddingLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdagrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(300, 300, vocab, fasttext_dict)\n",
    "loss_function = nn.CosineEmbeddingLoss(margin=0.0, reduction='sum')\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GRUModel(50, 50, vocab, fasttext_dict)\n",
    "# loss_function = nn.L1Loss(reduction = \"sum\")\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check overfit\n",
    "\n",
    "# definition_indices = vocab.words2indices(definitions)\n",
    "# words_in = 0\n",
    "# words_out = 0\n",
    "\n",
    "# import timeit\n",
    "# start = timeit.default_timer()\n",
    "# losses = []\n",
    "\n",
    "# batch_size = 128\n",
    "\n",
    "# for src_sents, tgt_word in batch_iter(training_data, batch_size, False):\n",
    "#     for i in range(300):\n",
    "#         model.zero_grad()\n",
    "#         x_lengths = [len(sent) for sent in src_sents]\n",
    "#         x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "#         init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "#         tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "#         y_array = model.embedding.source[0](torch.tensor(vocab.words2indices(tgt_word))).double()\n",
    "#         y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "#         loss = loss_function(y_pred, y_array, torch.tensor(1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step() \n",
    "#         losses.append(loss)\n",
    "#         if i % 100 == 0:\n",
    "#             print(i, loss)\n",
    "#     break\n",
    "    \n",
    "# stop = timeit.default_timer()\n",
    "\n",
    "# print('Time: ', stop - start)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(plt.plot([l.double() for l in losses][:1200]))\n",
    "\n",
    "# model.zero_grad()\n",
    "# x_lengths = [len(sent) for sent in src_sents]\n",
    "# x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "# init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "# tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "# y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "\n",
    "# validate_dict = dict([(w, model.embedding.source[0](torch.tensor(vocab[w])).numpy()) for w in set(words)])\n",
    "# print(len(validate_dict))\n",
    "\n",
    "# print(y_pred.shape)\n",
    "# for i in range(len(y_pred)):\n",
    "#     eval.top_ten_hundred(validate_dict, tgt_word[i], y_pred[i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f811c453e84a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdefinition_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords2indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefinitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwords_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwords_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "definition_indices = vocab.words2indices(definitions)\n",
    "words_in = 0\n",
    "words_out = 0\n",
    "\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "losses = []\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for src_sents, tgt_word in batch_iter(training_data, batch_size, False):\n",
    "        model.zero_grad()\n",
    "        x_lengths = [len(sent) for sent in src_sents]\n",
    "        x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "        init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "        tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "        y_array = model.embedding.source[0](torch.tensor(vocab.words2indices(tgt_word))).double()\n",
    "        y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "        loss = loss_function(y_pred, y_array, torch.tensor(1))\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    losses.append(loss)\n",
    "    print(epoch, loss, timeit.default_timer() - start)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(plt.plot([l.double() for l in losses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a18d5d41873f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlosses\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.plot([l.double() for l in losses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "x_lengths = [len(sent) for sent in src_sents]\n",
    "x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "\n",
    "validate_dict = dict([(w, model.embedding.source[0](torch.tensor(vocab[w])).detach().numpy()) for w in set(words)])\n",
    "print(len(validate_dict))\n",
    "\n",
    "print(y_pred.shape)\n",
    "for i in range(len(y_pred)):\n",
    "    eval.top_ten_hundred(validate_dict, tgt_word[i], y_pred[i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
