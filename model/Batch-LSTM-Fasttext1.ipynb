{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torchnlp.nn import Attention\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from model_embeddings import ModelEmbeddings\n",
    "from evaluator import Evaluator\n",
    "from vocab import Vocab, VocabEntry\n",
    "from utils import read_corpus, pad_sents, batch_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import fasttext.util\n",
    "ft = fasttext.load_model('../data/cc.en.300.bin')\n",
    "print(ft.get_dimension())\n",
    "fasttext.util.reduce_model(ft, 100)\n",
    "print(ft.get_dimension())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.11238685  0.03634512  0.06272481  0.09328334  0.07476841 -0.02136788\n",
      "  0.03564939  0.06776655 -0.00441328  0.06022001  0.00861067  0.03479845\n",
      "  0.0326384  -0.0459407   0.06693012 -0.03741141 -0.03098805  0.08830236\n",
      "  0.01789446 -0.00656855 -0.01357604  0.07741845 -0.06603968  0.00508138\n",
      " -0.02323393  0.03813258  0.04005572  0.02402163 -0.0174519   0.04406795\n",
      "  0.02939343  0.04441914  0.0360864   0.00104841  0.00701074 -0.03081188\n",
      "  0.02208788  0.03518926 -0.03685221 -0.0505261   0.04664233 -0.00106685\n",
      " -0.00812358  0.00405111  0.00817128  0.10642597  0.04042358  0.01328057\n",
      " -0.01919837 -0.02258547 -0.00679503 -0.01966     0.04078562 -0.01269364\n",
      "  0.04565648  0.00711938  0.03343869  0.02761208 -0.07310422 -0.02281325\n",
      "  0.0070655  -0.03781745  0.03798665  0.0331741   0.02306999 -0.04951117\n",
      "  0.03532698  0.02406708  0.00787704  0.0381924  -0.00423297  0.0656371\n",
      " -0.01671075 -0.00671035  0.02436351 -0.01863142  0.01288848  0.0659969\n",
      " -0.04377548  0.03321797  0.01051717 -0.02191262 -0.02064682 -0.01644386\n",
      " -0.02853318  0.01014358 -0.03771588  0.01777328  0.01407252 -0.01996439\n",
      " -0.01711383 -0.03785433 -0.04500444  0.03835989  0.02284297 -0.00086462\n",
      "  0.0209074  -0.02822562  0.0406603   0.00468414]\n"
     ]
    }
   ],
   "source": [
    "print(ft.get_word_vector('hdello'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "definitions = []\n",
    "unparsed_definition = []\n",
    "words = []\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data_train_words.txt') as f:\n",
    "    words += f.read().splitlines()\n",
    "    \n",
    "with open('../data/data_train_definitions.txt') as f:\n",
    "    unparsed_definition += f.read().splitlines()\n",
    "    definitions += [word_tokenize(a) for a in unparsed_definition]\n",
    "\n",
    "training_data = [(definitions[i], words[i]) for i in range(len(words))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word types: 23452, number of word types w/ frequency >= 0: 23452\n",
      "23456\n"
     ]
    }
   ],
   "source": [
    "eval = Evaluator()\n",
    "# fasttext_dict = eval.load_vectors(fname =\"../data/wiki-news-300d-1M-subword.vec\", max_line = 10000000)\n",
    "fasttext_dict = {}\n",
    "sub_fasttext_dict = {}\n",
    "#only train words in the dictionary\n",
    "for i in range(len(words)-1, -1, -1):\n",
    "    fasttext_dict[words[i]] = ft.get_word_vector(words[i])\n",
    "    sub_fasttext_dict[words[i]] = fasttext_dict[words[i]]\n",
    "        \n",
    "# high_freq_dict = eval.load_vectors(fname =\"../data/wiki-news-300d-1M-subword.vec\", max_line = 30000)\n",
    "# sub_fasttext_dict.update()\n",
    "\n",
    "src_sents = read_corpus('../data/data_train_definitions.txt')\n",
    "vocab = VocabEntry.from_corpus(src_sents, 1000000, 0)\n",
    "    \n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"../data/words_defs_dict.train\", \"wb\") as f:\n",
    "    pickle.dump((words, definitions, sub_fasttext_dict), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(words) == len(definitions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, src_pad_token_idx, non_trainable=True):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, src_pad_token_idx)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix)) #figure out what is here\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab, fasttext_dict):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (VocabEntry)\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        matrix_len = len(vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, self.embed_size))\n",
    "        words_found = 0\n",
    "        #print(len(vocab), weights_matrix.shape)\n",
    "        for word, index in vocab.word2id.items():\n",
    "            try:\n",
    "                weights_matrix[index] = np.array(fasttext_dict[word])\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[index] = np.random.normal(scale=0.6, size=(self.embed_size,))\n",
    "\n",
    "        # default values\n",
    "        src_pad_token_idx = vocab['<pad>']\n",
    "        self.source = create_emb_layer(weights_matrix, src_pad_token_idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GRUModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, vocab, fasttext_dict):\n",
    "#         super(GRUModel, self).__init__()\n",
    "#         self.hidden_size = hidden_size\n",
    "#         self.embedding = ModelEmbeddings(input_size, vocab, fasttext_dict)\n",
    "#         self.gru = nn.GRU(input_size, hidden_size)\n",
    "        \n",
    "#         self.linear = nn.Linear(self.hidden_size, self.hidden_size, bias = True)\n",
    "\n",
    "#     def forward(self, input_, hidden, lengths , dropout_rate = 0.3):\n",
    "#         embedded = self.embedding.source[0](input_)\n",
    "#         embedded = pack_padded_sequence(embedded, lengths)\n",
    "#         output, hidden = self.gru(embedded, hidden)\n",
    "#         dropout = nn.Dropout(dropout_rate)\n",
    "#         hidden_dropped = dropout(hidden.permute(1,0,2)) # you dont need dropout in validation\n",
    "#         projected = self.linear(hidden_dropped)\n",
    "#         return projected, hidden\n",
    "\n",
    "#     def initHidden(self, batch_size, device = None):\n",
    "#         return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, vocab, fasttext_dict):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "        self.vocab = vocab\n",
    "        self.embedding = ModelEmbeddings(input_size, vocab, fasttext_dict)\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, bidirectional = True)\n",
    "        self.linear = nn.Linear(self.hidden_size * 2, self.hidden_size, bias = True)\n",
    "        self.attention = Attention(self.hidden_size)\n",
    "\n",
    "    def forward(self, input_, hidden, lengths, dropout_rate = 0.3):\n",
    "        embedded = self.embedding.source[0](input_)\n",
    "        embedded = pack_padded_sequence(embedded, lengths)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        o, w = self.attention(h_n, output)\n",
    "        #print(h_n.shape, c_n.shape)\n",
    "        dropout = nn.Dropout(dropout_rate)\n",
    "        hidden_dropped = dropout(h_n.contiguous().view(1, -1, self.hidden_size * 2).permute(1,0,2)) # you dont need dropout in validation\n",
    "        projected = self.linear(hidden_dropped)\n",
    "        return projected, hidden\n",
    "\n",
    "    def initHidden(self, batch_size, device = None):\n",
    "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(100, 100, vocab, fasttext_dict)\n",
    "loss_function = nn.CosineEmbeddingLoss(margin=0.0, reduction='sum')\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GRUModel(50, 50, vocab, fasttext_dict)\n",
    "# loss_function = nn.L1Loss(reduction = \"sum\")\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check overfit\n",
    "\n",
    "# definition_indices = vocab.words2indices(definitions)\n",
    "# words_in = 0\n",
    "# words_out = 0\n",
    "\n",
    "# import timeit\n",
    "# start = timeit.default_timer()\n",
    "# losses = []\n",
    "\n",
    "# batch_size = 128\n",
    "\n",
    "# for src_sents, tgt_word in batch_iter(training_data, batch_size, False):\n",
    "#     for i in range(300):\n",
    "#         model.zero_grad()\n",
    "#         x_lengths = [len(sent) for sent in src_sents]\n",
    "#         x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "#         init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "#         tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "#         y_array = model.embedding.source[0](torch.tensor(vocab.words2indices(tgt_word))).double()\n",
    "#         y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "#         loss = loss_function(y_pred, y_array, torch.tensor(1))\n",
    "#         loss.backward()\n",
    "#         optimizer.step() \n",
    "#         losses.append(loss)\n",
    "#         if i % 100 == 0:\n",
    "#             print(i, loss)\n",
    "#     break\n",
    "    \n",
    "# stop = timeit.default_timer()\n",
    "\n",
    "# print('Time: ', stop - start)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# print(plt.plot([l.double() for l in losses][:1200]))\n",
    "\n",
    "# model.zero_grad()\n",
    "# x_lengths = [len(sent) for sent in src_sents]\n",
    "# x = vocab.to_input_tensor(src_sents, \"cpu\")\n",
    "# init_hidden = model.initHidden(len(src_sents), \"cpu\")\n",
    "# tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "# y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "\n",
    "# validate_dict = dict([(w, model.embedding.source[0](torch.tensor(vocab[w])).numpy()) for w in set(words)])\n",
    "# print(len(validate_dict))\n",
    "\n",
    "# print(y_pred.shape)\n",
    "# for i in range(len(y_pred)):\n",
    "#     eval.top_ten_hundred(validate_dict, tgt_word[i], y_pred[i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(33.2199, dtype=torch.float64, grad_fn=<SumBackward0>) 61.331675972032826\n",
      "1 tensor(33.1883, dtype=torch.float64, grad_fn=<SumBackward0>) 122.25149954605149\n",
      "2 tensor(33.2089, dtype=torch.float64, grad_fn=<SumBackward0>) 183.20772688701982\n",
      "3 tensor(33.1495, dtype=torch.float64, grad_fn=<SumBackward0>) 252.38635815604357\n",
      "4 tensor(33.1733, dtype=torch.float64, grad_fn=<SumBackward0>) 325.45885230600834\n",
      "5 tensor(33.1164, dtype=torch.float64, grad_fn=<SumBackward0>) 389.9770028950297\n",
      "6 tensor(33.2105, dtype=torch.float64, grad_fn=<SumBackward0>) 453.89217092405306\n",
      "7 tensor(33.2641, dtype=torch.float64, grad_fn=<SumBackward0>) 528.9641185130458\n",
      "8 tensor(33.1684, dtype=torch.float64, grad_fn=<SumBackward0>) 595.7289804510074\n",
      "9 tensor(33.0121, dtype=torch.float64, grad_fn=<SumBackward0>) 666.4902101700427\n",
      "10 tensor(33.0295, dtype=torch.float64, grad_fn=<SumBackward0>) 736.4332790100016\n",
      "11 tensor(32.6346, dtype=torch.float64, grad_fn=<SumBackward0>) 807.7851033370243\n",
      "12 tensor(32.4849, dtype=torch.float64, grad_fn=<SumBackward0>) 879.9165674030082\n",
      "13 tensor(32.7999, dtype=torch.float64, grad_fn=<SumBackward0>) 950.9382531760493\n",
      "14 tensor(32.7300, dtype=torch.float64, grad_fn=<SumBackward0>) 1021.1793489310076\n",
      "15 tensor(32.5961, dtype=torch.float64, grad_fn=<SumBackward0>) 1085.7807857040316\n",
      "16 tensor(32.9986, dtype=torch.float64, grad_fn=<SumBackward0>) 1146.9182457569987\n",
      "17 tensor(32.6913, dtype=torch.float64, grad_fn=<SumBackward0>) 1215.4848021950456\n",
      "18 tensor(32.4340, dtype=torch.float64, grad_fn=<SumBackward0>) 1285.2646061340347\n",
      "19 tensor(32.8256, dtype=torch.float64, grad_fn=<SumBackward0>) 1353.7572632050142\n",
      "20 tensor(32.4180, dtype=torch.float64, grad_fn=<SumBackward0>) 1422.376859548036\n",
      "21 tensor(32.5525, dtype=torch.float64, grad_fn=<SumBackward0>) 1481.9722119810176\n",
      "22 tensor(32.0409, dtype=torch.float64, grad_fn=<SumBackward0>) 1546.4351384540205\n",
      "23 tensor(32.2660, dtype=torch.float64, grad_fn=<SumBackward0>) 1616.1504107270157\n",
      "24 tensor(33.3370, dtype=torch.float64, grad_fn=<SumBackward0>) 1684.238422585011\n",
      "25 tensor(32.3853, dtype=torch.float64, grad_fn=<SumBackward0>) 1749.4483824490453\n",
      "26 tensor(32.5630, dtype=torch.float64, grad_fn=<SumBackward0>) 1820.8672496899962\n",
      "27 tensor(32.6324, dtype=torch.float64, grad_fn=<SumBackward0>) 1884.191177936038\n",
      "28 tensor(32.7535, dtype=torch.float64, grad_fn=<SumBackward0>) 1948.6363157990272\n",
      "29 tensor(31.9536, dtype=torch.float64, grad_fn=<SumBackward0>) 2009.9748425120488\n",
      "30 tensor(32.4625, dtype=torch.float64, grad_fn=<SumBackward0>) 2072.755928076047\n",
      "31 tensor(32.5623, dtype=torch.float64, grad_fn=<SumBackward0>) 2135.1420314060524\n",
      "32 tensor(32.5163, dtype=torch.float64, grad_fn=<SumBackward0>) 2200.335271380027\n",
      "33 tensor(31.5597, dtype=torch.float64, grad_fn=<SumBackward0>) 2265.631394972035\n",
      "34 tensor(31.8861, dtype=torch.float64, grad_fn=<SumBackward0>) 2328.1458141180337\n",
      "35 tensor(31.3884, dtype=torch.float64, grad_fn=<SumBackward0>) 2388.769837076019\n",
      "36 tensor(32.1101, dtype=torch.float64, grad_fn=<SumBackward0>) 2448.9094216080266\n",
      "37 tensor(32.1689, dtype=torch.float64, grad_fn=<SumBackward0>) 2509.1681868919986\n",
      "38 tensor(31.4364, dtype=torch.float64, grad_fn=<SumBackward0>) 2569.8001313260174\n",
      "39 tensor(31.7791, dtype=torch.float64, grad_fn=<SumBackward0>) 2629.804310330015\n",
      "40 tensor(31.9896, dtype=torch.float64, grad_fn=<SumBackward0>) 2690.4748607799993\n",
      "41 tensor(31.8952, dtype=torch.float64, grad_fn=<SumBackward0>) 2759.687068035011\n",
      "42 tensor(31.5667, dtype=torch.float64, grad_fn=<SumBackward0>) 2841.703117669036\n",
      "43 tensor(31.8474, dtype=torch.float64, grad_fn=<SumBackward0>) 2911.9727827210445\n",
      "44 tensor(31.4563, dtype=torch.float64, grad_fn=<SumBackward0>) 2976.2408402599976\n",
      "45 tensor(31.5327, dtype=torch.float64, grad_fn=<SumBackward0>) 3046.060452683014\n",
      "46 tensor(31.8245, dtype=torch.float64, grad_fn=<SumBackward0>) 3118.281965867034\n",
      "47 tensor(32.4492, dtype=torch.float64, grad_fn=<SumBackward0>) 3186.8824690330075\n",
      "48 tensor(31.7189, dtype=torch.float64, grad_fn=<SumBackward0>) 3256.0225931640016\n",
      "49 tensor(31.8250, dtype=torch.float64, grad_fn=<SumBackward0>) 3326.108389213041\n",
      "50 tensor(31.7595, dtype=torch.float64, grad_fn=<SumBackward0>) 3388.6002715009963\n",
      "51 tensor(32.0446, dtype=torch.float64, grad_fn=<SumBackward0>) 3452.174613303039\n",
      "52 tensor(32.1972, dtype=torch.float64, grad_fn=<SumBackward0>) 3517.1703770640306\n",
      "53 tensor(31.5543, dtype=torch.float64, grad_fn=<SumBackward0>) 3584.3154947170406\n",
      "54 tensor(32.3906, dtype=torch.float64, grad_fn=<SumBackward0>) 3647.068318516016\n",
      "55 tensor(31.5663, dtype=torch.float64, grad_fn=<SumBackward0>) 3706.7263205840136\n",
      "56 tensor(31.7576, dtype=torch.float64, grad_fn=<SumBackward0>) 3775.443440055009\n",
      "57 tensor(31.2681, dtype=torch.float64, grad_fn=<SumBackward0>) 3842.9340561410063\n",
      "58 tensor(31.0369, dtype=torch.float64, grad_fn=<SumBackward0>) 3907.503423734044\n",
      "59 tensor(31.8180, dtype=torch.float64, grad_fn=<SumBackward0>) 3973.8679176490405\n",
      "60 tensor(31.6458, dtype=torch.float64, grad_fn=<SumBackward0>) 4048.5643341750256\n",
      "61 tensor(31.6636, dtype=torch.float64, grad_fn=<SumBackward0>) 4131.532316533034\n",
      "62 tensor(31.3237, dtype=torch.float64, grad_fn=<SumBackward0>) 4195.139340712049\n",
      "63 tensor(31.2449, dtype=torch.float64, grad_fn=<SumBackward0>) 4258.42703828204\n",
      "64 tensor(31.8681, dtype=torch.float64, grad_fn=<SumBackward0>) 4322.283283465018\n",
      "65 tensor(31.6549, dtype=torch.float64, grad_fn=<SumBackward0>) 4387.299297969032\n",
      "66 tensor(31.3714, dtype=torch.float64, grad_fn=<SumBackward0>) 4453.590175991005\n",
      "67 tensor(31.6701, dtype=torch.float64, grad_fn=<SumBackward0>) 4518.950275662006\n",
      "68 tensor(31.1766, dtype=torch.float64, grad_fn=<SumBackward0>) 4579.451363295026\n"
     ]
    }
   ],
   "source": [
    "definition_indices = vocab.words2indices(definitions)\n",
    "words_in = 0\n",
    "words_out = 0\n",
    "\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "losses = []\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "for epoch in range(5000):\n",
    "    for src_sents, tgt_word in batch_iter(training_data, batch_size, False):\n",
    "        model.zero_grad()\n",
    "        x_lengths = [len(sent) for sent in src_sents]\n",
    "        x = vocab.to_input_tensor(src_sents, device)\n",
    "        init_hidden = model.initHidden(len(src_sents), device)\n",
    "        tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "        y_array = model.embedding.source[0](torch.tensor(vocab.words2indices(tgt_word))).double()\n",
    "        y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "        loss = loss_function(y_pred, y_array, torch.tensor(1))\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    losses.append(loss)\n",
    "    print(epoch, loss, timeit.default_timer() - start)\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "\n",
    "print('Time: ', stop - start)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(plt.plot([l.double() for l in losses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "print(plt.plot([l.double() for l in losses]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.zero_grad()\n",
    "x_lengths = [len(sent) for sent in src_sents]\n",
    "x = vocab.to_input_tensor(src_sents, device)\n",
    "init_hidden = model.initHidden(len(src_sents), device)\n",
    "tag_scores = model.forward(x, init_hidden, x_lengths)\n",
    "y_pred = tag_scores[0].squeeze(dim = 1).double()\n",
    "\n",
    "validate_dict = dict([(w, model.embedding.source[0](torch.tensor(vocab[w])).detach().numpy()) for w in set(words)])\n",
    "print(len(validate_dict))\n",
    "\n",
    "print(y_pred.shape)\n",
    "for i in range(len(y_pred)):\n",
    "    eval.top_ten_hundred(validate_dict, tgt_word[i], y_pred[i].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
