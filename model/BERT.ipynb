{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/yuloucn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import io\n",
    "\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "from typing import List, Tuple, Dict, Set, Union\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from nltk import word_tokenize\n",
    "import pickle\n",
    "import timeit\n",
    "from scipy import spatial\n",
    "\n",
    "from evaluator import Evaluator\n",
    "from vocab import Vocab, VocabEntry\n",
    "from utils import read_corpus, pad_sents, batch_iter_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of word types: 23437, number of word types w/ frequency >= 0: 23437\n"
     ]
    }
   ],
   "source": [
    "words, defs, ft_dict = pickle.load( open( \"../data/words_defs_dict_1M.train\", \"rb\" ))\n",
    "\n",
    "vocab = VocabEntry.from_corpus(defs, 1000000, 0)\n",
    "for w in ft_dict:\n",
    "    vocab.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_emb_layer(weights_matrix, src_pad_token_idx, non_trainable=True):\n",
    "    num_embeddings, embedding_dim = weights_matrix.shape\n",
    "    emb_layer = nn.Embedding(num_embeddings, embedding_dim, src_pad_token_idx)\n",
    "    emb_layer.weight.data.copy_(torch.from_numpy(weights_matrix)) #figure out what is here\n",
    "    if non_trainable:\n",
    "        emb_layer.weight.requires_grad = False\n",
    "    return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class ModelEmbeddings(nn.Module): \n",
    "    \"\"\"\n",
    "    Class that converts input words to their embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab, fasttext_dict):\n",
    "        \"\"\"\n",
    "        Init the Embedding layers.\n",
    "\n",
    "        @param embed_size (int): Embedding size (dimensionality)\n",
    "        @param vocab (VocabEntry)\n",
    "        \"\"\"\n",
    "        super(ModelEmbeddings, self).__init__()\n",
    "\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "        matrix_len = len(vocab)\n",
    "        weights_matrix = np.zeros((matrix_len, self.embed_size))\n",
    "        words_found = 0\n",
    "        #print(len(vocab), weights_matrix.shape)\n",
    "        for word, index in vocab.word2id.items():\n",
    "            try:\n",
    "                weights_matrix[index] = np.array(fasttext_dict[word])\n",
    "                words_found += 1\n",
    "            except KeyError:\n",
    "                weights_matrix[index] = np.random.normal(scale=0.6, size=(self.embed_size,))\n",
    "\n",
    "        # default values\n",
    "        src_pad_token_idx = vocab['<pad>']\n",
    "        self.source = create_emb_layer(weights_matrix, src_pad_token_idx, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = map(float, tokens[1:])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseDictionary(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab, ft_dict, freeze_bert = False):\n",
    "        super(ReverseDictionary, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        \n",
    "        self.ft_embedding = ModelEmbeddings(embed_dim, vocab, ft_dict)\n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "#         Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        #Classification layer\n",
    "        self.lstm_fasttext = nn.LSTM(embed_dim, hidden_dim, batch_first = True)\n",
    "        self.lin_layer = nn.Linear(hidden_dim+768, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, ft_input, lengths, bert_input, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        embedded = self.ft_embedding.source[0](ft_input).transpose(1,0)\n",
    "\n",
    "        embedded = pack_padded_sequence(embedded, lengths, batch_first=True)\n",
    "        output, (cn, hn) = self.lstm_fasttext(embedded)\n",
    "        \n",
    "        cont_reps, _ = self.bert_layer(bert_input, attention_mask = attn_masks)\n",
    "        cls_rep = cont_reps[:, 0]\n",
    "\n",
    "        toLinear = torch.cat([cls_rep, cn.squeeze(0)], 1)\n",
    "\n",
    "        #Obtaining the representation of [CLS] head\n",
    "        \n",
    "        #feed cls_rep to -> fasttext layer\n",
    "        projected = self.lin_layer(toLinear)\n",
    "\n",
    "        return projected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ReverseDictionary(300, 300, vocab, ft_dict)\n",
    "loss_function = nn.L1Loss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = .001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_sents = vocab.words2indices(defs)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "max_len = max(len(x) for x in int_sents)\n",
    "sents_ft_ids = int_sents\n",
    "sents_bert_ids = []\n",
    "masks = []\n",
    "for d in defs:\n",
    "    tokens = ['[CLS]'] + d + ['[SEP]']\n",
    "    padded_tokens = tokens + ['[PAD]' for _ in range(max_len - len(tokens))]\n",
    "    attn_mask = [1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
    "    seg_ids = [0 for _ in range(len(padded_tokens))]\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(padded_tokens)\n",
    "    sents_bert_ids.append(token_ids)\n",
    "    masks.append(attn_mask)\n",
    "    \n",
    "\n",
    "assert(len(sents_bert_ids) == len(defs))\n",
    "assert(len(sents_bert_ids) == len(masks))\n",
    "assert(len(masks) == len(words))\n",
    "data = [(defs[i], len(defs[i]), sents_bert_ids[i], masks[i], words[i]) for i in range(len(defs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2443, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.3441, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(2.4338, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(1.2702, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.6273, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(1.6615, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.7969, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.6948, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.5741, dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-474e818ea537>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss_cum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlossavg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_cum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_cum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/local_nmt/lib/python3.5/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/local_nmt/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "losses = []\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "for epoch in range(5000):\n",
    "    loss_cum = []\n",
    "    for batch_defs, batch_lengths, batch_bert_ids, batch_masks, batch_words in \\\n",
    "                    batch_iter_bert(data, BATCH_SIZE, shuffle=False):\n",
    "        model.zero_grad()\n",
    "        #print(batch_lengths)\n",
    "        batch_ft_ids = vocab.to_input_tensor(batch_defs, device = \"cpu\")\n",
    "        batch_bert_ids = torch.tensor(batch_bert_ids, device = \"cpu\")\n",
    "        batch_masks = torch.tensor(batch_masks, device = \"cpu\")\n",
    "        tag_scores = model.forward(batch_ft_ids, batch_lengths, batch_bert_ids, batch_masks)\n",
    "        \n",
    "        y_pred = tag_scores[0].double()\n",
    "        y_indices = torch.tensor([vocab[i] for i in batch_words])\n",
    "        y_array = model.ft_embedding.source[0](y_indices).double()\n",
    "        #print(y_array.shape)\n",
    "\n",
    "        loss = loss_function(y_pred, y_array)\n",
    "        print(loss)\n",
    "        loss_cum.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    lossavg = sum(loss_cum)/len(loss_cum)\n",
    "    losses.append(loss)\n",
    "    #print(epoch, lossavg, timeit.default_timer() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2355, dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yuloucn/opt/miniconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  del sys.path[0]\n",
      "/Users/yuloucn/opt/miniconda3/envs/local_nmt/lib/python3.5/site-packages/ipykernel_launcher.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2434, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.1535, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.1348, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0883, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.1155, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.2149, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.1107, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0871, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0705, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0659, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0638, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0551, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0524, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0500, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.1201, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0577, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0544, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0481, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0427, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0380, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0368, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0367, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0350, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0326, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0305, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0293, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0287, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0290, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0282, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0269, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0250, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0249, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0239, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0226, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0228, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0224, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0218, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0237, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0246, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0234, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0224, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0223, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0201, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0185, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0187, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0186, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0187, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0186, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0188, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0185, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0181, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0182, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0182, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0184, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0187, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0168, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0168, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0190, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0172, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0163, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0159, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0158, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0165, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0163, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0160, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0153, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0152, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0161, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0155, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0151, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0149, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0140, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0141, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0144, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0147, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0146, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0131, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0139, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0126, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0137, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0130, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0127, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0120, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0120, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0132, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0131, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0133, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0145, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0135, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0122, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0126, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0136, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0128, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0128, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0123, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0127, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0125, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0125, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0124, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0131, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0115, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0108, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0130, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0122, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0113, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0119, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0127, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0118, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0112, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0109, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0111, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0120, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0119, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0114, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0113, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0120, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0116, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0117, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0113, dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0121, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0119, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0123, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0126, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0124, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0118, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0112, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0109, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0112, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0116, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0116, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0112, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0114, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0109, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0105, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0111, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0110, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0107, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0111, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0101, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0106, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0106, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0110, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0111, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0106, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0101, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0109, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0102, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0104, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0100, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0098, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0105, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0104, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0097, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0097, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0099, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0100, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0092, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0101, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0096, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0088, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0098, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0097, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0092, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0094, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0096, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0094, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0100, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0098, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0096, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0100, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0099, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0092, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0092, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0088, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0094, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0085, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0097, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0085, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0085, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0086, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0097, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0099, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0088, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0091, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0093, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0095, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0088, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0084, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0090, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0083, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0087, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0083, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0084, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0086, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0086, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0079, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0086, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0083, dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0081, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0079, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0083, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0089, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0072, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0080, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0080, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0080, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0079, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0080, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0072, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0072, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0072, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0081, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0075, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0073, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0077, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0076, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0070, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0069, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0071, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0078, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0068, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0063, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0061, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0063, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0066, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0061, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0074, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0065, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0063, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0050, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0050, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0067, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0064, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0061, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0062, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0058, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0052, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0050, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0060, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0059, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0057, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0055, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0054, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0053, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0056, dtype=torch.float64, grad_fn=<L1LossBackward>)\n",
      "tensor(0.0051, dtype=torch.float64, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "losses = []\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "\n",
    "loss_cum = []\n",
    "for batch_defs, batch_lengths, batch_bert_ids, batch_masks, batch_words in \\\n",
    "                batch_iter_bert(data, BATCH_SIZE, shuffle=False):\n",
    "    for i in range(500):\n",
    "        model.zero_grad()\n",
    "        #print(batch_lengths)\n",
    "        batch_ft_ids = vocab.to_input_tensor(batch_defs, device = \"cpu\")\n",
    "        batch_bert_ids = torch.tensor(batch_bert_ids, device = \"cpu\")\n",
    "        batch_masks = torch.tensor(batch_masks, device = \"cpu\")\n",
    "        tag_scores = model.forward(batch_ft_ids, batch_lengths, batch_bert_ids, batch_masks)\n",
    "\n",
    "        y_pred = tag_scores[0].double()\n",
    "        y_indices = torch.tensor([vocab[i] for i in batch_words])\n",
    "        y_array = model.ft_embedding.source[0](y_indices).double()\n",
    "        #print(y_array.shape)\n",
    "\n",
    "        loss = loss_function(y_pred, y_array)\n",
    "        print(loss)\n",
    "        loss_cum.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "    lossavg = sum(loss_cum)/len(loss_cum)\n",
    "    losses.append(loss)\n",
    "    #print(epoch, lossavg, timeit.default_timer() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents_bert_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-1c6f079dd5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtag_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents_bert_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msents_ft_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtag_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.unsqueeze(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print(y_pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sents_bert_id' is not defined"
     ]
    }
   ],
   "source": [
    "eval = Evaluator()\n",
    "model.zero_grad()\n",
    "\n",
    "for epoch in range(5000):\n",
    "    loss_cum = []\n",
    "    for batch_defs, batch_lengths, batch_bert_ids, batch_masks, batch_words in \\\n",
    "                    batch_iter_bert(data, BATCH_SIZE, shuffle=False):\n",
    "        model.zero_grad()\n",
    "        #print(batch_lengths)\n",
    "        batch_ft_ids = vocab.to_input_tensor(batch_defs, device = \"cpu\")\n",
    "        batch_bert_ids = torch.tensor(batch_bert_ids, device = \"cpu\")\n",
    "        batch_masks = torch.tensor(batch_masks, device = \"cpu\")\n",
    "        tag_scores = model.forward(batch_ft_ids, batch_lengths, batch_bert_ids, batch_masks)\n",
    "        \n",
    "        y_pred = tag_scores[0].double()\n",
    "        y_indices = torch.tensor([vocab[i] for i in batch_words])\n",
    "        y_array = model.ft_embedding.source[0](y_indices).double()\n",
    "        eval.top_ten_hundred(ft_dict, words[i], y_pred.detach().numpy())\n",
    "    print(np.linalg.norm(ft_dict[words[i]]-y_pred.detach().numpy()))\n",
    "#     print(np.linalg.norm(ft_dict[words[i]]-y_pred.detach().numpy()))\n",
    "#     print(sorted(ft_dict.keys(), key=lambda word: spatial.distance.cosine(ft_dict[word], y_pred.detach().numpy())))\n",
    "#     print(ft_dict['fault'].shape, y_pred.detach().numpy().shape)\n",
    "#     print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
